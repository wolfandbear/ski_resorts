{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4670aabd-775e-4016-84ea-dfc1f2992b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#movie review data (imdb data source) sentiment analysis\n",
    "#based on Python Machine Learning Book code/Ch8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1849fb9-d772-4dd3-802e-2fa880bac7c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "1  OK... so... I really like Kris Kristofferson a...          0\n",
       "2  ***SPOILER*** Do not read this, if you think a...          0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import movie review data\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv('movie_data.csv', encoding='utf-8')\n",
    "df = df.rename(columns={\"0\": \"review\", \"1\": \"sentiment\"})\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "303db3d0-4743-4588-80bf-1f0317503cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some Sentiment Analysis Concepts / Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3b212b0-efff-4aec-98e5-42b49c0ad119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 6, 'sun': 4, 'is': 1, 'shining': 3, 'weather': 8, 'sweet': 5, 'and': 0, 'one': 2, 'two': 7}\n",
      "[[0 1 0 1 1 0 1 0 0]\n",
      " [0 1 0 0 0 1 1 0 1]\n",
      " [2 3 2 1 1 1 2 1 1]]\n"
     ]
    }
   ],
   "source": [
    "#bag of words concepts\n",
    "#transform sentences into bag of words vocabulary and sparse vectors\n",
    "#call fit_transform on the count vectorizer\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count = CountVectorizer()\n",
    "docs = np.array([\n",
    "        'The sun is shining',\n",
    "        'The weather is sweet',\n",
    "        'The sun is shining, the weather is sweet, and one and one is two'])\n",
    "bag = count.fit_transform(docs)\n",
    "\n",
    "#this is the vocabulary learned from the data set\n",
    "#the key is the word and the val is the indices in the array\n",
    "#so 'is' is the 2nd col index 1\n",
    "print(count.vocabulary_)\n",
    "\n",
    "#this is what the bag of words sparse array looks like\n",
    "#Those values in the feature vectors are also called the raw term frequencies: \n",
    "#tf (t,d)â€”the number of times a term t occurs in a document d.\n",
    "#so'is' tf(t,d) = 3 in document 3\n",
    "print(bag.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "053dc0ee-0cf9-4a08-9a7f-44fbd0dab947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.43370786 0.         0.55847784 0.55847784 0.\n",
      "  0.43370786 0.         0.        ]\n",
      " [0.         0.43370786 0.         0.         0.         0.55847784\n",
      "  0.43370786 0.         0.55847784]\n",
      " [0.50238645 0.44507629 0.50238645 0.19103892 0.19103892 0.19103892\n",
      "  0.29671753 0.25119322 0.19103892]]\n",
      "[[0.         0.43370786 0.         0.55847784 0.55847784 0.\n",
      "  0.43370786 0.         0.        ]\n",
      " [0.         0.43370786 0.         0.         0.         0.55847784\n",
      "  0.43370786 0.         0.55847784]\n",
      " [0.50238645 0.44507629 0.50238645 0.19103892 0.19103892 0.19103892\n",
      "  0.29671753 0.25119322 0.19103892]]\n"
     ]
    }
   ],
   "source": [
    "#then we apply the tfidf (term frequency-inverse document frequency)\n",
    "#to the output from the CountVectorizer\n",
    "\n",
    "#transformer TfidfTransformer takes the raw term frequencies\n",
    "#from CountVectorizer as input and transforms them into tf-idfs:\n",
    "\n",
    "#idf(t,d) = log( (1+n_d)/(1+df(d,t)) )\n",
    "#for 'is' idf = log((1+3)/(1+3) ) = log(1) = 0\n",
    "#where n_d total docs\n",
    "#df(d,t) num of docs d cntain term t\n",
    "\n",
    "#then the tf-idf(t,d) = tf(t,d) x (idf(t,d)+1)\n",
    "#for 'is' tf-idf(t,d) = (3) x (0+1) = 3 (not normalized)\n",
    "#L2-normalization, which returns a vector of \n",
    "#length 1 by dividing an un-normalized feature vector v by its L2-norm\n",
    "#v_norm = v_notnormalized / (sum of squares of not normalized values)^0.5\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer(use_idf=True, norm='l2', smooth_idf=True)\n",
    "\n",
    "print(tfidf.fit_transform(count.fit_transform(docs)).toarray())\n",
    "#which is the same as applying it to 'bag' from above\n",
    "print(tfidf.fit_transform(bag).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17b310bc-84e3-45f2-b75b-3877d6d55ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean the Data from the Movie Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "578eed69-d357-4c21-9064-9274d2857c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In 1974, the teenager Martha Moxley (Maggie Grace) moves to the high-class area of Belle Haven, Greenwich, Connecticut. On the Mischief Night, eve of Halloween, she was murdered in the backyard of her house and her murder remained unsolved. Twenty-two years later, the writer Mark Fuhrman (Christopher Meloni), who is a former LA detective that has fallen in disgrace for perjury in O.J. Simpson trial and moved to Idaho, decides to investigate the case with his partner Stephen Weeks (Andrew Mitchell) with the purpose of writing a book. The locals squirm and do not welcome them, but with the support of the retired detective Steve Carroll (Robert Forster) that was in charge of the investigation in the 70\\'s, they discover the criminal and a net of power and money to cover the murder.<br /><br />\"Murder in Greenwich\" is a good TV movie, with the true story of a murder of a fifteen years old girl that was committed by a wealthy teenager whose mother was a Kennedy. The powerful and rich family used their influence to cover the murder for more than twenty years. However, a snoopy detective and convicted perjurer in disgrace was able to disclose how the hideous crime was committed. The screenplay shows the investigation of Mark and the last days of Martha in parallel, but there is a lack of the emotion in the dramatization. My vote is seven.<br /><br />Title (Brazil): Not Available'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#here is a sample review\n",
    "df.loc[0, 'review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5ca1afd-bcad-4156-9f37-0c13497a8d95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"OK... so... I really like Kris Kristofferson and his usual easy going delivery of lines in his movies. Age has helped him with his soft spoken low energy style and he will steal a scene effortlessly. But, Disappearance is his misstep. Holy Moly, this was a bad movie! <br /><br />I must give kudos to the cinematography and and the actors, including Kris, for trying their darndest to make sense from this goofy, confusing story! None of it made sense and Kris probably didn't understand it either and he was just going through the motions hoping someone would come up to him and tell him what it was all about! <br /><br />I don't care that everyone on this movie was doing out of love for the project, or some such nonsense... I've seen low budget movies that had a plot for goodness sake! This had none, zilcho, nada, zippo, empty of reason... a complete waste of good talent, scenery and celluloid! <br /><br />I rented this piece of garbage for a buck, and I want my money back! I want my 2 hours back I invested on this Grade F waste of my time! Don't watch this movie, or waste 1 minute of your valuable time while passing through a room where it's playing or even open up the case that is holding the DVD! Believe me, you'll thank me for the advice!\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[1, 'review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe20f0ca-a937-4e39-a4f7-aec5ad0a9e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use regex to clean html and emoticons in data\n",
    "\n",
    "#find the emoticons in the text \n",
    "#based on the syntax and append to the end of the text\n",
    "\n",
    "import re\n",
    "def preprocessor(text):\n",
    "    #remove html by regex using '<[^>]*>' by replacing it with ''\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    #find the emoticons\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    #remove all non-word characters with regex [\\W], \n",
    "    #convert to lower case and then join the emoticon at the end\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', ''))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a24fc40-9e8e-4722-8240-cb9871a60750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'was a bad movie i mus'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test the preprocessor to remove the html tags adn the nose of the emoticon\n",
    "preprocessor(\"was a bad movie! <br /><br />I mus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "383a64a8-d454-47ff-adc6-79a595119567",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Apply the preprocessor to the entire data set\n",
    "df['review'] = df['review'].apply(preprocessor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18d64b68-b40d-49b7-b38a-f7cc835abbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concept before applying the model - text to tokens\n",
    "\n",
    "#converting text into 'tokens' is splitting the text into indiv. elements\n",
    "#can be indiv words 1-gram, or 2 words 2-gram etc\n",
    "\n",
    "#use porter stemmer from nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4670d6e-c661-4358-b98d-9dc26e402555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['runners', 'like', 'running', 'and', 'thus', 'they', 'run']\n",
      "['runner', 'like', 'run', 'and', 'thu', 'they', 'run']\n"
     ]
    }
   ],
   "source": [
    "#check the tokenizer function\n",
    "print(tokenizer('runners like running and thus they run'))\n",
    "#check the tokenizer_porter function (just the stems)\n",
    "print(tokenizer_porter('runners like running and thus they run'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18cabb75-fddc-429d-bde9-8c0b866ac6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/alisonmichan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Concept before applying the model - text to tokens\n",
    "#using stopwords to remove irrelevant small words\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#use the engllish stop words from nltk\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff60b79a-1592-4549-80aa-f9c590bd5841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before stopwords: a runner likes running and runs a lot\n",
      "after stopwords:  ['runner', 'like', 'run', 'run', 'lot']\n"
     ]
    }
   ],
   "source": [
    "#see how the stopwords works\n",
    "print('before stopwords: a runner likes running and runs a lot')\n",
    "print('after stopwords: ', [w for w in tokenizer_porter\\\n",
    "       ('a runner likes running and runs a lot')\\\n",
    "       if w not in stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9a4d1d5-16b1-4f63-b44c-9fde8c290f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data train /test 50/50 of the data set\n",
    "\n",
    "#df.shape is 50k by 2\n",
    "X_train = df.loc[:25000, 'review'].values\n",
    "y_train = df.loc[:25000, 'sentiment'].values\n",
    "X_test = df.loc[25000:, 'review'].values\n",
    "y_test = df.loc[25000:, 'sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ddc6c6e3-90cc-4419-ac36-3157d36eb5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages for ML model\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "067cd038-46f1-466d-9a35-b22493884fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/alisonmichan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#let's apply niavely a set of params without optimization\n",
    "\n",
    "#tfid vectorizer def\n",
    "#tfidf = TfidfVectorizer(strip_accents=None,\n",
    "#                        lowercase=None,\n",
    "#                        preprocessor=None)\n",
    "\n",
    "#let's set the following for tfidf\n",
    "#vect_ngram is 1 (1-word elements in the vectorizer)\n",
    "#stop_words - stop\n",
    "#tokenizer - tokenizer porter stemmer\n",
    "\n",
    "\n",
    "#define stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "#use the engllish stop words from nltk\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#use porter stemmer from nltk and define tokenizer_porter\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]\n",
    "\n",
    "#stop defined above, tokenizer_porter defined above\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=None,\n",
    "                        tokenizer=tokenizer_porter,\n",
    "                        stop_words=stop)                       \n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c94d952-84e4-4421-a115-2118007f650d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25001,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fda75137-5b8f-408d-892d-e787c02859e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alisonmichan/opt/anaconda3/envs/py37/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', \"it'\", 'onc', 'onli', 'ourselv', \"she'\", \"should'v\", 'themselv', 'thi', 'veri', 'wa', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
      "  % sorted(inconsistent)\n"
     ]
    }
   ],
   "source": [
    "#fit/transform the tfidf on the train data and apply to test data\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0baa544-360f-4e6b-b834-b5a8ff2d6a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  89.264 %\n"
     ]
    }
   ],
   "source": [
    "#classifier is logistic regression with liblinear\n",
    "clf = LogisticRegression(solver='liblinear', penalty='l2', C=1.0)\n",
    "clf.fit(X_train_tfidf,y_train)\n",
    "\n",
    "#prediction and accuracy\n",
    "y_pred = clf.predict(X_test_tfidf)\n",
    "test_accuracy = clf.score(X_test_tfidf,y_test)\n",
    "print('Test Accuracy: ', f'{100*test_accuracy:.3f}','%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc37078d-fae7-4640-892c-e1e3434f9d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Best parameter set: {'clf__C': 10.0, 'clf__penalty': 'l2', 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <function tokenizer at 0x7fd967043320>}\n",
      "CV Accuracy: 0.897\n",
      "Test Accuracy: 0.899\n"
     ]
    }
   ],
   "source": [
    "#let's use grid search to find the best hyperparams based on accuracy as a metric\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=None,\n",
    "                        preprocessor=None)\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf),\n",
    "                     ('clf', LogisticRegression(solver='liblinear'))])\n",
    "\n",
    "small_param_grid = [{'vect__ngram_range': [(1, 1)],\n",
    "                     'vect__stop_words': [None],\n",
    "                     'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "                     'clf__penalty': ['l2'],\n",
    "                     'clf__C': [1.0, 10.0]},\n",
    "                    {'vect__ngram_range': [(1, 1)],\n",
    "                     'vect__stop_words': [stop, None],\n",
    "                     'vect__tokenizer': [tokenizer],\n",
    "                     'vect__use_idf':[False],\n",
    "                     'vect__norm':[None],\n",
    "                     'clf__penalty': ['l2'],\n",
    "                  'clf__C': [1.0, 10.0]},\n",
    "              ]\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, small_param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=5,\n",
    "                           verbose=1,\n",
    "                           n_jobs=-1)\n",
    "\n",
    "\n",
    "\n",
    "gs_lr_tfidf.fit(X_train, y_train)\n",
    "print(f'Best parameter set: {gs_lr_tfidf.best_params_}')\n",
    "print(f'CV Accuracy: {gs_lr_tfidf.best_score_:.3f}')\n",
    "clf = gs_lr_tfidf.best_estimator_\n",
    "print(f'Test Accuracy: {clf.score(X_test, y_test):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd53004-e729-45ac-9f25-45074df6ec78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
